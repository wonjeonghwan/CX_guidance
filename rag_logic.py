# rag_logic.py (ë©”ì¸ ì‹¤í–‰ íŒŒì¼)
from prompts.service_suggest_prompt import refund_prompt
from retrievers.chroma_retriever import get_chroma_retriever
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda
from DB import update_response, get_messages_by_customer

# 1. ëª¨ë¸ ë° ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.4)
retriever = get_chroma_retriever()

# 2. ì²´ì¸ êµ¬ì„±
chain = (
    {"context": retriever, "question": RunnableLambda(lambda x: x)}
    | refund_prompt
    | llm
    | StrOutputParser()
)

# 3. ì‹¤í–‰ìš© í•¨ìˆ˜
def generate_response(user_question: str, number: int, customer_id: str):
    # 1. ì´ì „ ëŒ€í™” ì´ë ¥ ì¡°íšŒ
    past_logs = get_messages_by_customer(customer_id)
    history_context = "\n".join(
        f"Q: {msg}\nA: {res}" for _, msg, res in past_logs if res
    )

    # 2. ëŒ€í™” ì´ë ¥ + í˜„ì¬ ì§ˆë¬¸ì„ ë¬¶ì–´ì„œ ì „ë‹¬
    composite_question = f"{history_context}\n\n[í˜„ì¬ ì§ˆë¬¸]\n{user_question}"

    # 3. RAG ì‹¤í–‰ ë° DB ì €ì¥
    response = chain.invoke(composite_question)
    update_response(number, response)
    return response

# 4. í…ŒìŠ¤íŠ¸ìš© ì½”ë“œ
if __name__ == "__main__":
    print("=== RAG ë´‡ í…ŒìŠ¤íŠ¸ ===")
    q1 = "ì•„ì´ê°€ í¥ë¯¸ë¥¼ ëŠë¼ì§€ ëª»í•´ìš”, ì•„ì´ì—ê²Œ ë§ëŠ” í•™ìŠµ ë°©ì‹ì´ ì•„ë‹ˆì—ìš”. í•™ìŠµí•  ì‹œê°„ì´ ì—†ì–´ìš”"
    q2 = "ì•„ì´ê°€ ë„ˆë¬´ ì–´ë ¤ì›Œí•´ì„œ ì´ìš© ëª»í•´ìš”. í™˜ë¶ˆ ê°€ëŠ¥í•œê°€ìš”?"
    q3 = "ë¼ì´ë¸Œí´ë˜ìŠ¤ëŠ” ì˜ í•˜ëŠ”ë° GVë ˆìŠ¨ì´ë‚˜ ìˆ™ì œë¥¼ ì˜ ì˜ ì•ˆí•˜ë ¤ê³  í•˜ë„¤ìš”...ì§€ê¸ˆ í™˜ë¶ˆí•˜ê²Œ ë˜ë©´ ë¹„ìš© ì²˜ë¦¬ê°€ ì–´ë–»ê²Œ ë ê¹Œìš”?"
    q4 = "ì‚¬ë‘ì€ ë­˜ê¹Œìš”?"
    for q in [q1, q2, q3, q4]:
        print(f"\nğŸ™‹ ì‚¬ìš©ì: {q}")
        print("ğŸ¤– ë´‡ ì‘ë‹µ:", generate_response(q))
